from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# T·∫°o Spark session
spark = SparkSession.builder \
    .appName("pgAdmin_to_Hudi_ETL") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog") \
    .config("spark.hadoop.fs.s3a.access.key", "admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "password") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
    .enableHiveSupport() \
    .getOrCreate()

# C·∫•u h√¨nh PostgreSQL
jdbc_url = "jdbc:postgresql://host.docker.internal:5432/Lakehouse_demo"
jdbc_properties = {
    "user": "postgres",
    "password": "931943",
    "driver": "org.postgresql.Driver"
}

# Danh s√°ch b·∫£ng v√† c·∫•u h√¨nh t∆∞∆°ng ·ª©ng
tables = [
    {
        "source_table": "public.car_sales",
        "hudi_table": "car_sales",
        "recordkey": "id",
        "precombine": "year",
        "path": "s3a://carsalesdata/car_sales"
    },
    {
        "source_table": "public.car_sales_updated",
        "hudi_table": "car_sales_updated",
        "recordkey": "id",
        "precombine": "year",
        "path": "s3a://carsalesdata/car_sales_updated"
    }
]

# L·∫∑p qua t·ª´ng b·∫£ng ƒë·ªÉ ingest
for t in tables:
    print(f"\nüöó Ingesting table: {t['hudi_table']}")

    df = spark.read.jdbc(url=jdbc_url, table=t["source_table"], properties=jdbc_properties)
    df = df.select([F.col(c).alias(c.replace(" ", "_").lower()) for c in df.columns])

    df.write.format("hudi") \
        .option("hoodie.table.name", t["hudi_table"]) \
        .option("hoodie.datasource.write.recordkey.field", t["recordkey"]) \
        .option("hoodie.datasource.write.precombine.field", t["precombine"]) \
        .option("hoodie.datasource.write.operation", "insert") \
        .option("hoodie.datasource.hive_sync.enable", "true") \
        .option("hoodie.datasource.hive_sync.mode", "hms") \
        .option("hoodie.datasource.hive_sync.metastore.uris", "thrift://hive-metastore:9083") \
        .option("hoodie.datasource.hive_sync.table", t["hudi_table"]) \
        .option("hoodie.datasource.hive_sync.database", "default") \
        .option("hoodie.datasource.hive_sync.support_timestamp", "true") \
        .option("path", t["path"]) \
        .mode("overwrite") \
        .save()

    # Ki·ªÉm tra Hive Metastore ƒë√£ sync ch∆∞a
    try:
        spark.sql(f"DESC TABLE default.{t['hudi_table']}")
        print(f"‚úÖ Hive table `{t['hudi_table']}` is signup.")
    except Exception:
        print(f"‚ö†Ô∏è Hive table `{t['hudi_table']}` none exist, need create...")
        spark.sql(f"""
        CREATE TABLE default.{t['hudi_table']}
        USING hudi
        LOCATION '{t["path"]}'
        """)
        print(f"‚úÖ hive table create complete `{t['hudi_table']}`.")

print("\n‚úÖ Ingestion to Hudi and Hive sync complete.")